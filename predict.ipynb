{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "In this notebook, we will load the `model.h5` that the `train.ipynb` yielded. Then we will use it to classify the input images. The folder with the input images can be set by the `input_folder` variable. We will save the classification to a CSV file on disk. Finally, we will plot a few examples for a brief manual sanity check by the scientists.\n",
    "\n",
    "**Note**:\n",
    "\n",
    "We are currently using TensorFlow 2.0 which is currently in beta state. So it is expected and ok that there are warnings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## settings, imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import imageio\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import gitignored.remap as remap\n",
    "import gitignored.config as cfg\n",
    "\n",
    "import bodestm.util as util\n",
    "\n",
    "input_folder = 'gitignored/to_predict/*.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data set from generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show which data will be classified\n",
    "content_of_inputfolder = glob.glob(input_folder)\n",
    "pd.DataFrame({\"input_files\": content_of_inputfolder}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_generator(\n",
    "    generator=lambda: util.gen_filenames(content_of_inputfolder),\n",
    "    output_types=tf.string,\n",
    "    output_shapes=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the model from disk\n",
    "\n",
    "It is crucial to pick the right model file. E.g. if the model was trained in the cloud and one executes this notebook on a local system, he has to transfer the `model.h5` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(cfg.gitignored_path + \"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(\n",
    "    ds.map(util.parse_image).batch(32),\n",
    "    # docu says that \"do not specify the batch_size is your data is in the form of .., dataset, ...\"\n",
    "    # but it must be explizitly set to \"None\" even if the docu says the default values is \"None\". Otherwise there will be an error.\n",
    "    batch_size=None\n",
    ")\n",
    "print(\"shape of predictions: {}\".format(predictions.shape))\n",
    "print('succesfully predicted {} images'.format(len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = []\n",
    "predicted_class_prob = []\n",
    "\n",
    "for prediction in predictions:\n",
    "    predicted_class.append(np.argmax(prediction))\n",
    "    predicted_class_prob.append(np.max(prediction))\n",
    "    \n",
    "df_predictions = pd.DataFrame({\n",
    "    \"file_name\": content_of_inputfolder,\n",
    "    \"predicted_class_int\": predicted_class,\n",
    "    \"predicted_class_prob\": predicted_class_prob\n",
    "})\n",
    "\n",
    "df_predictions[\"predicted_class_ext\"] = remap.remap_cross_back(df_predictions.predicted_class_int)\n",
    "\n",
    "df_predictions.to_csv('gitignored/to_predict/predictions.csv')\n",
    "\n",
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot predicted images with classification\n",
    "\n",
    "We plot a few images for a brief sanity check by the scientists. The prediction is shown in the external as well as in the internal nomenclature. Also, the confidence of the network for the classification is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df_predictions)\n",
    "\n",
    "if n > 50:\n",
    "    n = 50\n",
    "\n",
    "cols = 4\n",
    "rows = (n//cols) + 1\n",
    "idx = 1\n",
    "\n",
    "ax = None\n",
    "fig = plt.figure(figsize=(cols * 4, rows * 3))\n",
    "\n",
    "for i in range(n):\n",
    "    ax = fig.add_subplot(rows, cols, idx)\n",
    "    img = imageio.imread(df_predictions.file_name[i])\n",
    "    predicted_class = df_predictions.predicted_class_int[i]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(\"prediction: {} (int {})\".format(df_predictions.predicted_class_ext[i], predicted_class))\n",
    "    prob = df_predictions.predicted_class_prob[i]\n",
    "    ax.set_xlabel(\"confidence {:.3f}\".format(prob))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    idx += 1\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
